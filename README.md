# Fun with optimizers

I tried to implement the famous optimizers in different deep learning frameworks. 

The frameworks that I used are as follows:

- Tensorflow
- Theano
- Pytorch
- Keras
- Numpy (Technically not a deep learning framework but a great framework to make learn low level neural network)

The optimization algorithms that I implemented here are as follows:

* Normal Gradiant descent (GD)
* GD with Momentum
* Adagrad
* RMSprop
* RMSprop with Momentum
* Adam

There is a fantastic explanation of these optimizers written by Sebastian Ruder. You can find more here. http://ruder.io/optimizing-gradient-descent/index.html#momentum
